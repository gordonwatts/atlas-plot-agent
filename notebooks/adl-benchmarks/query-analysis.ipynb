{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdd0669",
   "metadata": {},
   "source": [
    "# Direct Query Question Analysis\n",
    "\n",
    "The 8 `adl-benchmark` questions were listed [here](README.md). The details of the models used to run over them can be found [here](models.yaml). This notebook is an analysis of the results.\n",
    "\n",
    "The following was the *process* used:\n",
    "\n",
    "1. Each question and model is done in isolation - nothing is learned or retained from one question to the next.\n",
    "1. A prompt is assembled with the following contents:\n",
    "    - [Some prompt text](https://github.com/gordonwatts/atlas-plot-agent/blob/d3a8363630234f16dc3248a092d5ff92d4c57b58/notebooks/adl-benchmarks/direct-query-config.yaml#L8)\n",
    "    - [Hint files](https://github.com/gordonwatts/hep-programming-hints/tree/1.0.0)\n",
    "    - The question (the question is sourced from the [README.md](README.md) file)\n",
    "1. The LLM returns with some text and python code.\n",
    "1. The python code is run in a docker container\n",
    "1. If `png` files are produced as output, then the question is considered successfully answered (see the below section on hallucinations), and we move onto the next question.\n",
    "1. If not, then a new prompt is assembled:\n",
    "    - [Different prompt text](https://github.com/gordonwatts/atlas-plot-agent/blob/d3a8363630234f16dc3248a092d5ff92d4c57b58/notebooks/adl-benchmarks/direct-query-config.yaml#L30)\n",
    "    - [Hint files](https://github.com/gordonwatts/hep-programming-hints/tree/1.0.0)\n",
    "    - The question\n",
    "    - The previous code\n",
    "    - The error output from the code\n",
    "1. The LLM replies again with an error analysis and code, at which point we go back to the step above where we run the code in the docker container.\n",
    "1. This loop was run, for these steps, up to a max of three times.\n",
    "\n",
    "All the back and forth is recorded in the files in this repo, like [`direct-question-01.md`](direct-question-01.md).\n",
    "\n",
    "One thing to note: the performance is dependent on the hint files! Some errors below could be avoided by better hint files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e3721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd08e21",
   "metadata": {},
   "source": [
    "Parameterize the funning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c42ae",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "results_dir = \"direct-query\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c13e60",
   "metadata": {},
   "source": [
    "The CSV results are written into all the `direct-question-xx.md` files found in this same directory. Use the following command to build a `results.csv` file which we can read in:\n",
    "\n",
    "```bash\n",
    "python ./notebooks/adl-benchmarks/query-results-to-pandas.py ./notebooks/adl-benchmarks/direct-question-*.md\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26527b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python query-results-to-pandas.py results/{results_dir}/direct-question-*.md\n",
    "results_csv = Path(f\"results/{results_dir}/results.csv\")\n",
    "if results_csv.exists():\n",
    "    results_csv.unlink()\n",
    "Path(\"results.csv\").rename(results_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(results_csv)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e40657",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "You could argue this should be done in a support file. You'd be right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f881a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the question number into something we can easily reason about\n",
    "data['question'] = data['source_file'].apply(lambda x: int(re.search(r'direct-question-(\\d+)', x).group(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca318456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models.yaml so that we can get from the detailed model name to the \"nick-name\"\n",
    "with open(\"models.yaml\", \"r\") as f:\n",
    "    models = yaml.safe_load(f)[\"models\"]\n",
    "\n",
    "nickname_lookup = {models[name][\"model_name\"]: name for name in models.keys()}\n",
    "# data['model_nickname'] = data['Model'].map(nickname_lookup)\n",
    "data[\"model_nickname\"] = data[\"Model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b1fd4",
   "metadata": {},
   "source": [
    "## Model Cost\n",
    "\n",
    "These models have a wide range of costs! The individual costs need to be pulled from the `models.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f230c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model names and output costs\n",
    "model_costs = [(name, info.get('output_cost_per_million', 'N/A')) for name, info in models.items()]\n",
    "\n",
    "model_info = pd.DataFrame(model_costs, columns=[\"model_name\", \"output_cost_per_million\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70809010",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info_sorted = model_info.sort_values(by=\"output_cost_per_million\", ascending=False)\n",
    "model_info_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcbb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model costs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_info_sorted['model_name'], model_info_sorted['output_cost_per_million'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Output Cost per Million ($)')\n",
    "plt.title('Model Output Cost per Million Tokens')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec71b8eb",
   "metadata": {},
   "source": [
    "Look for duplicate information - which can be lethal later one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ea41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "# Find duplicate (question, model_nickname) pairs\n",
    "dupes = data[data.duplicated(subset=[\"question\", \"model_nickname\"], keep=False)]\n",
    "print(\"Duplicate rows for (question, model_nickname):\")\n",
    "print(dupes[[\"question\", \"model_nickname\", \"Result\", \"Model\"]])\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abdde0c",
   "metadata": {},
   "source": [
    "## Success and Attempts\n",
    "\n",
    "Below is a complex grid plot that attempts to summarize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Prepare pivot table for grid, with columns sorted by model cost\n",
    "sorted_nicknames = model_info_sorted['model_name'].tolist()\n",
    "pivot = data.pivot(index='question', columns='model_nickname', values='Result')\n",
    "pivot = pivot.reindex(columns=sorted_nicknames)\n",
    "\n",
    "# Prepare attempts table for text labels\n",
    "attempts_pivot = data.pivot(index='question', columns='model_nickname', values='Attempts')\n",
    "attempts_pivot = attempts_pivot.reindex(columns=sorted_nicknames)\n",
    "\n",
    "# Map results to colors\n",
    "color_map = pivot.map(lambda x: '#90ee90' if x == 'Success' else '#ffcccb')  # light green/red\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "for (i, question) in enumerate(pivot.index):\n",
    "    for (j, model) in enumerate(pivot.columns):\n",
    "        color = color_map.loc[question, model]\n",
    "        ax.add_patch(plt.Rectangle((j, i), 1, 1, color=color))\n",
    "        # Show number of attempts as text label\n",
    "        attempts = attempts_pivot.loc[question, model]\n",
    "        ax.text(\n",
    "            j + 0.5,\n",
    "            i + 0.5,\n",
    "            f\"{attempts} - {'OK' if pivot.loc[question, model] == 'Success' else 'Fail'}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "ax.set_xticks(np.arange(len(pivot.columns)) + 0.5)\n",
    "ax.set_xticklabels(pivot.columns, rotation=45, ha='right')\n",
    "ax.set_yticks(np.arange(len(pivot.index)) + 0.5)\n",
    "ax.set_yticklabels(pivot.index)\n",
    "ax.set_xlim(0, len(pivot.columns))\n",
    "ax.set_ylim(0, len(pivot.index))\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Model (sorted by cost)')\n",
    "ax.set_ylabel('Question')\n",
    "ax.set_title('Question Performance (attempts and ultimate result)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c1330",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "- There is a definite break between questions 4 and 5, and 5 and 6.\n",
    "- Note that `gtp-oss-20b` sometimes fails with 1 attempt. That is because the model returns no code with the prompt.\n",
    "- On average, performance is flash-ish across costs, which I find amazing, except for `gpt-5`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42c0b5",
   "metadata": {},
   "source": [
    "## Questions 4, 5, and 6\n",
    "\n",
    "Repeating questions 4, 5, and 6 here for reference:\n",
    "\n",
    "4. Plot the ETmiss of events that have at least two jets with pT > 40 GeV in the rucio dataset mc23_13p6TeV:mc23_13p6TeV.801167.Py8EG_A14NNPDF23LO_jj_JZ2.deriv.DAOD_PHYSLITE.e8514_e8528_a911_s4114_r15224_r15225_p6697.\n",
    "5. Plot the ETmiss of events that have an opposite-charge muon pair with an invariant mass between 60 and 120 GeV in the rucio dataset mc23_13p6TeV:mc23_13p6TeV.513109.MGPy8EG_Zmumu_FxFx3jHT2bias_SW_CFilterBVeto.deriv.DAOD_PHYSLITE.e8514_e8528_s4162_s4114_r14622_r14663_p6697.\n",
    "6. For events with at least three jets, plot the pT of the trijet four-momentum that has the invariant mass closest to 172.5 GeV in each event and plot the maximum b-tagging discriminant value among the jets in this trijet in the rucio dataset mc23_13p6TeV:mc23_13p6TeV.601237.PhPy8EG_A14_ttbar_hdamp258p75_allhad.deriv.DAOD_PHYSLITE.e8514_s4369_r16083_p6697.\n",
    "\n",
    "- Question 4 is a 2 objet (ETmiss and jets) question. The aggregation is just counting, which is then cut on.\n",
    "- Question 5 is a 2 object (ETMiss and muons) question. The filtering is 2-object (opposite sign). The aggregation is the invariant mass calculation, which is then cut on.\n",
    "- Question 6 is a 1 object (jets) question. But two plots. The b-tagging discriminate value in ATLAS requires a fair amount of extra code. Aggregation is 3-jet combinatorics, and then you have to pick the best b-tagging discriminate from that.\n",
    "\n",
    "These questions certainly are increasingly more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb481ab",
   "metadata": {},
   "source": [
    "## Hallucinations\n",
    "\n",
    "While I did cursory inspections of the models output, the above plot makes it clear there might be some more careful investigation required. In particular, `coder-large` and `gtp-5-nano` both seem to have successes on question 8, which is very rare! In general, it seems that the change from question 4, to 5, and to 6 causes trouble.\n",
    "\n",
    "The below table is one that lists cases where the models made the plots, but the plots are incorrect. The code is:\n",
    "\n",
    "* **\"H\"** Straight up hallucination\n",
    "* **\"K\"** Made some comment in `stdout` or `stderr` which indicated it knew something was wrong and needed a human to peer over its shoulder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65616e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_review_all = {\n",
    "    \"direct-query\": [\n",
    "        (\"5\", \"gpt-4o\", \"H\"),  # Bad calc\n",
    "        (\"5\", \"o4-mini\", \"H\"),  # ignores muons\n",
    "        (\"6\", \"gpt-5-mini\", \"H\"),  # only one plot\n",
    "        (\"6\", \"gpt-5-nano\", \"H\"),  # only one plot (and wrong)\n",
    "        (\"6\", \"Qwen3-Coder-480B\", \"H\"),  # only one plot\n",
    "        (\"6\", \"deepseek-chat-v3-0324\", \"H\"),  # empty plots\n",
    "        (\"7\", \"gpt-5-nano\", \"H\"),  # bad calc and used loops!\n",
    "        (\"7\", \"o4-mini\", \"H\"),  # Put in zeros\n",
    "        (\"7\", \"llama-4-maverick-400B\", \"H\"),  # plotted jets\n",
    "    ],\n",
    "    \"plan-query\": [],\n",
    "}\n",
    "\n",
    "# Pick the right set of results for this current work.\n",
    "human_review = human_review_all[results_dir]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be087492",
   "metadata": {},
   "source": [
    "Adding this information to the table so we can make another \"nice\" grid plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hallucination and corrected result columns\n",
    "hallucination_map = {(int(q), m): h for q, m, h in human_review}\n",
    "data['hallucination'] = data.apply(lambda row: hallucination_map.get((row['question'], row['model_nickname']), ''), axis=1)\n",
    "def correct_result(row):\n",
    "    if row['hallucination'] in ['H', 'K'] and row['Result'] == 'Success':\n",
    "        return 'Fail'\n",
    "    return row['Result']\n",
    "data['Result_with_hallucination'] = data.apply(correct_result, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae2047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Prepare pivot table for grid, with columns sorted by model cost\n",
    "sorted_nicknames = model_info_sorted[\"model_name\"].tolist()\n",
    "pivot = data.pivot(\n",
    "    index=\"question\", columns=\"model_nickname\", values=\"Result_with_hallucination\"\n",
    ")\n",
    "pivot = pivot.reindex(columns=sorted_nicknames)\n",
    "\n",
    "# Prepare attempts table for text labels\n",
    "attempts_pivot = data.pivot(\n",
    "    index=\"question\", columns=\"model_nickname\", values=\"Attempts\"\n",
    ")\n",
    "attempts_pivot = attempts_pivot.reindex(columns=sorted_nicknames)\n",
    "\n",
    "# Prepare hallucination table for annotation\n",
    "hallucination_pivot = data.pivot(\n",
    "    index=\"question\", columns=\"model_nickname\", values=\"hallucination\"\n",
    ")\n",
    "hallucination_pivot = hallucination_pivot.reindex(columns=sorted_nicknames)\n",
    "\n",
    "# Map results to colors\n",
    "color_map = pivot.map(\n",
    "    lambda x: \"#90ee90\" if x == \"Success\" else \"#ffcccb\"\n",
    ")  # light green/red\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "for i, question in enumerate(pivot.index):\n",
    "    for j, model in enumerate(pivot.columns):\n",
    "        color = color_map.loc[question, model]\n",
    "        ax.add_patch(plt.Rectangle((j, i), 1, 1, color=color))\n",
    "        # Show number of attempts as text label\n",
    "        attempts = attempts_pivot.loc[question, model]\n",
    "        ax.text(\n",
    "            j + 0.5,\n",
    "            i + 0.5,\n",
    "            f\"{attempts} - {'OK' if pivot.loc[question, model] == 'Success' else 'Fail'}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        # If hallucination is non-empty, show letter in upper right\n",
    "        hallucination = hallucination_pivot.loc[question, model]\n",
    "        if isinstance(hallucination, str) and hallucination.strip():\n",
    "            ax.text(\n",
    "                j + 0.95,\n",
    "                i + 0.15,\n",
    "                hallucination,\n",
    "                ha=\"right\",\n",
    "                va=\"top\",\n",
    "                fontsize=14,\n",
    "                fontweight=\"bold\",\n",
    "                color=\"black\",\n",
    "            )\n",
    "\n",
    "ax.set_xticks(np.arange(len(pivot.columns)) + 0.5)\n",
    "ax.set_xticklabels(pivot.columns, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(np.arange(len(pivot.index)) + 0.5)\n",
    "ax.set_yticklabels(pivot.index)\n",
    "ax.set_xlim(0, len(pivot.columns))\n",
    "ax.set_ylim(0, len(pivot.index))\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel(\"Model (sorted by cost)\")\n",
    "ax.set_ylabel(\"Question\")\n",
    "ax.set_title(\"Question Performance (attempts, ultimate result, hallucination)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb708d",
   "metadata": {},
   "source": [
    "## Question Cost\n",
    "\n",
    "Finally, we can look at the cost of a success or a fail for each question. This must be taken with a grain of salt - this is a single try with each LLM and question. However, given cost is driven by size of input and size of output, it is likely that this is accurate (just not the pass/fail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Prepare pivot table for grid, with columns sorted by model cost\n",
    "sorted_nicknames = model_info_sorted[\"model_name\"].tolist()\n",
    "pivot = data.pivot(\n",
    "    index=\"question\", columns=\"model_nickname\", values=\"Result_with_hallucination\"\n",
    ")\n",
    "pivot = pivot.reindex(columns=sorted_nicknames)\n",
    "\n",
    "# Prepare cost table for text labels (convert to cents)\n",
    "cost_pivot = data.pivot(\n",
    "    index=\"question\", columns=\"model_nickname\", values=\"EstimatedCost\"\n",
    ")\n",
    "cost_pivot = cost_pivot.reindex(columns=sorted_nicknames)\n",
    "\n",
    "# Prepare hallucination table for annotation\n",
    "hallucination_pivot = data.pivot(\n",
    "    index=\"question\", columns=\"model_nickname\", values=\"hallucination\"\n",
    ")\n",
    "hallucination_pivot = hallucination_pivot.reindex(columns=sorted_nicknames)\n",
    "\n",
    "# Map results to colors\n",
    "color_map = pivot.map(\n",
    "    lambda x: \"#90ee90\" if x == \"Success\" else \"#ffcccb\"\n",
    ")  # light green/red\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "for i, question in enumerate(pivot.index):\n",
    "    for j, model in enumerate(pivot.columns):\n",
    "        color = color_map.loc[question, model]\n",
    "        ax.add_patch(plt.Rectangle((j, i), 1, 1, color=color))\n",
    "        # Show cost in cents as text label\n",
    "        cost = cost_pivot.loc[question, model]\n",
    "        ax.text(\n",
    "            j + 0.5,\n",
    "            i + 0.5,\n",
    "            f\"{cost:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "ax.set_xticks(np.arange(len(pivot.columns)) + 0.5)\n",
    "ax.set_xticklabels(pivot.columns, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(np.arange(len(pivot.index)) + 0.5)\n",
    "ax.set_yticklabels(pivot.index)\n",
    "ax.set_xlim(0, len(pivot.columns))\n",
    "ax.set_ylim(0, len(pivot.index))\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_ylabel(\"Question\")\n",
    "ax.set_title(\"Question Performance (cost in cents)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664945bf",
   "metadata": {},
   "source": [
    "A few things to note here:\n",
    "\n",
    "- Given the way that the code works, the cost goes as the number of attempts.\n",
    "- An expensive model that gets it right on the first try is cheaper than a less expensive model that has to take several iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2afca55",
   "metadata": {},
   "source": [
    "Which model is the least expensive successful model on each question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the cheapest successful model for each question\n",
    "cheapest_success = (\n",
    "    data[data['Result_with_hallucination'] == 'Success']\n",
    "    .groupby('question')\n",
    "    .apply(lambda df: df.loc[df['EstimatedCost'].idxmin()])\n",
    ")\n",
    "\n",
    "cheapest_df = cheapest_success[['question', 'model_nickname', 'EstimatedCost']].reset_index(drop=True)\n",
    "cheapest_df.rename(columns={'model_nickname': 'CheapestModel', 'EstimatedCost': 'CheapestCost'}, inplace=True)\n",
    "cheapest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59894c4",
   "metadata": {},
   "source": [
    "It would be nice to know the answer to the question: if you had to pick one model, which is the cheapest? The answer, however, is gpt-5 currently, as it is the only model that successfully answered all questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
